# Distilbert

DistilBERT is a small, fast, cheap and light Transformer model trained by distilling Bert base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of Bertâ€™s performances as measured on the GLUE language understanding benchmark.